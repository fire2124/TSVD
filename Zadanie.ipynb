{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.linalg import Vectors\n",
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# preds = df.select(['label', 'features']) \\\n",
    "#                             .df.map(lambda line: (line[1], line[0]))\n",
    "# metrics = MulticlassMetrics(preds)\n",
    "\n",
    "    # Confusion Matrix\n",
    "# print(metrics.confusionMatrix().toArray())```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#x = numpy.random.uniform(0.0, 5.0, 250)\n",
    "\n",
    "#plt.hist(x, 5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|Accident_Severity_Day_of_Week|    1|    2|    3|    4|    5|    6|    7|\n",
      "+-----------------------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|                            2|23942|29487|30915|30958|31556|34732|28784|\n",
      "|                            1|  649|  542|  500|  495|  539|  741|  737|\n",
      "+-----------------------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#vzorka_cela.crosstab(\"Accident_Severity\", \"Day_of_Week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#vzorka_cela.saveAsTextFile('small_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vzorka_cela.write.format('com.databricks.spark.csv').mode('append').option(\"header\", \"true\").save('C:/Users/FIlip/Documents/TSVD/Zadanie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 20:53:40) [MSC v.1500 64 bit (AMD64)]\n",
      "Spark version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"example23\")\n",
    "\n",
    "print(u'Python version ' + sys.version)\n",
    "print(u'Spark version ' + sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Zadanie\").getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nacitanie\n",
    "data=spark.read.format('csv').options(header='true', inferSchema='true').load('C:/Users/FIlip/Documents/TSVD/Zadanie/dataset/Accidents.csv')\n",
    "data1=spark.read.format('csv').options(header='true', inferSchema='true').load('C:/Users/FIlip/Documents/TSVD/Zadanie/dataset/Vehicles.csv')\n",
    "data2=spark.read.format('csv').options(header='true', inferSchema='true').load('C:/Users/FIlip/Documents/TSVD/Zadanie/dataset/Casualties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data1.withColumnRenamed(\"Accident_Index\",\"ID\")\n",
    "data2=data2.withColumnRenamed(\"Accident_Index\",\"IDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Accident_Index: string, Location_Easting_OSGR: int, Location_Northing_OSGR: int, Longitude: double, Latitude: double, Police_Force: int, Accident_Severity: int, Number_of_Vehicles: int, Number_of_Casualties: int, Date: string, Day_of_Week: int, Time: string, Local_Authority_(District): int, Local_Authority_(Highway): string, 1st_Road_Class: int, 1st_Road_Number: int, Road_Type: int, Speed_limit: int, Junction_Detail: int, Junction_Control: int, 2nd_Road_Class: int, 2nd_Road_Number: int, Pedestrian_Crossing-Human_Control: int, Pedestrian_Crossing-Physical_Facilities: int, Light_Conditions: int, Weather_Conditions: int, Road_Surface_Conditions: int, Special_Conditions_at_Site: int, Carriageway_Hazards: int, Urban_or_Rural_Area: int, Did_Police_Officer_Attend_Scene_of_Accident: int, LSOA_of_Accident_Location: string, Vehicle_Type: int, Towing_and_Articulation: int, Vehicle_Manoeuvre: int, Vehicle_Location-Restricted_Lane: int, Junction_Location: int, Skidding_and_Overturning: int, Hit_Object_in_Carriageway: int, Vehicle_Leaving_Carriageway: int, Hit_Object_off_Carriageway: int, 1st_Point_of_Impact: int, Was_Vehicle_Left_Hand_Drive?: int, Journey_Purpose_of_Driver: int, Sex_of_Driver: int, Age_of_Driver: int, Age_Band_of_Driver: int, Engine_Capacity_(CC): int, Propulsion_Code: int, Age_of_Vehicle: int, Driver_IMD_Decile: int, Driver_Home_Area_Type: int, Casualty_Reference: int, Casualty_Class: int, Sex_of_Casualty: int, Age_of_Casualty: int, Age_Band_of_Casualty: int, Casualty_Severity: int, Pedestrian_Location: int, Pedestrian_Movement: int, Car_Passenger: int, Bus_or_Coach_Passenger: int, Pedestrian_Road_Maintenance_Worker: int, Casualty_Type: int, Casualty_Home_Area_Type: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spojenie dat a vymazanie potom tych duplikatych ID stlpcov\n",
    "merge = data.join(data1, data.Accident_Index == data1.ID)\n",
    "merge.drop(\"ID\")\n",
    "full_merge = merge.join(data2, merge.Accident_Index == data2.IDE)\n",
    "full_merge=full_merge.drop(\"IDE\")\n",
    "full_merge=full_merge.drop(\"ID\")\n",
    "full_merge=full_merge.drop(\"Vehicle_Reference\")\n",
    "\n",
    "len(full_merge.columns)\n",
    "full_merge\n",
    "#print(full_merge.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# čiže ci cieľovy atribut je Accident_Severity a ma 3 hodnoty : 1, 2, 3 - jedna je smrtelna 2 je že važna ale nezomrel a 3 je že ľahka... tak som 3 replacol na 2 aby bolo len smrtelna\n",
    "# a niesmrtelna\n",
    "from pyspark.sql.functions import when\n",
    "newsdf = full_merge.withColumn(\"Accident_Severity\", when(full_merge[\"Accident_Severity\"] == 3, 2).otherwise(full_merge[\"Accident_Severity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4287593"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cize sampling  som si vytiahol koľko je počet mrtvych a koľko počet čo prežili nehodu a podľa toho som v tokom pomere zmenšil tie data na 5%\n",
    "newsdf.registerTempTable(\"TempTable\")\n",
    "mrtvy = sqlContext.sql('SELECT * FROM TempTable WHERE Accident_Severity = 1')\n",
    "mrtvy_pocet = mrtvy.count()\n",
    "zivy = sqlContext.sql('SELECT * FROM TempTable WHERE Accident_Severity = 2')\n",
    "zivy_pocet = zivy.count()\n",
    "\n",
    "vzorka_mrtvy = mrtvy.sampleBy(\"Accident_Severity\", fractions = {1: 0.05}, seed = 0)\n",
    "vzorka_zivy = zivy.sampleBy(\"Accident_Severity\", fractions = {2: 0.05}, seed = 0)\n",
    "\n",
    "#tu ich nazad spojim a s tym datasetom treba pracovať ďalej\n",
    "vzorka_cela = vzorka_mrtvy.union(vzorka_zivy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rozdelenie na trenovaciu a testovaciu \n",
    "training_data, test_data = vzorka_cela.randomSplit([0.6, 0.4], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214577"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vzorka_cela.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Accident_Index=u'200522DB43935', Location_Easting_OSGR=385650, Location_Northing_OSGR=273200, Longitude=-2.212142, Latitude=52.356639, Police_Force=22, Accident_Severity=1, Number_of_Vehicles=2, Number_of_Casualties=3, Date=u'03/04/2005', Day_of_Week=1, Time=u'15:41', Local_Authority_(District)=278, Local_Authority_(Highway)=u'E10000034', 1st_Road_Class=3, 1st_Road_Number=450, Road_Type=6, Speed_limit=60, Junction_Detail=0, Junction_Control=-1, 2nd_Road_Class=-1, 2nd_Road_Number=0, Pedestrian_Crossing-Human_Control=0, Pedestrian_Crossing-Physical_Facilities=0, Light_Conditions=1, Weather_Conditions=1, Road_Surface_Conditions=1, Special_Conditions_at_Site=0, Carriageway_Hazards=0, Urban_or_Rural_Area=2, Did_Police_Officer_Attend_Scene_of_Accident=1, LSOA_of_Accident_Location=u'E01032431', Vehicle_Type=9, Towing_and_Articulation=0, Vehicle_Manoeuvre=17, Vehicle_Location-Restricted_Lane=0, Junction_Location=0, Skidding_and_Overturning=0, Hit_Object_in_Carriageway=0, Vehicle_Leaving_Carriageway=0, Hit_Object_off_Carriageway=0, 1st_Point_of_Impact=1, Was_Vehicle_Left_Hand_Drive?=1, Journey_Purpose_of_Driver=15, Sex_of_Driver=1, Age_of_Driver=58, Age_Band_of_Driver=9, Engine_Capacity_(CC)=1799, Propulsion_Code=1, Age_of_Vehicle=7, Driver_IMD_Decile=1, Driver_Home_Area_Type=1, Casualty_Reference=1, Casualty_Class=1, Sex_of_Casualty=1, Age_of_Casualty=22, Age_Band_of_Casualty=5, Casualty_Severity=1, Pedestrian_Location=0, Pedestrian_Movement=0, Car_Passenger=0, Bus_or_Coach_Passenger=0, Pedestrian_Road_Maintenance_Worker=-1, Casualty_Type=4, Casualty_Home_Area_Type=1)]\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "print(vzorka_cela.take(1))\n",
    "print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Accident_Index=u'200522DB43935', Location_Easting_OSGR=385650, Location_Northing_OSGR=273200, Longitude=-2.212142, Latitude=52.356639, Police_Force=22, Accident_Severity=1, Number_of_Vehicles=2, Number_of_Casualties=3, Date=u'03/04/2005', Day_of_Week=1, Time=u'15:41', Local_Authority_(District)=278, Local_Authority_(Highway)=u'E10000034', 1st_Road_Class=3, 1st_Road_Number=450, Road_Type=6, Speed_limit=60, Junction_Detail=0, Junction_Control=-1, 2nd_Road_Class=-1, 2nd_Road_Number=0, Pedestrian_Crossing-Human_Control=0, Pedestrian_Crossing-Physical_Facilities=0, Light_Conditions=1, Weather_Conditions=1, Road_Surface_Conditions=1, Special_Conditions_at_Site=0, Carriageway_Hazards=0, Urban_or_Rural_Area=2, Did_Police_Officer_Attend_Scene_of_Accident=1, LSOA_of_Accident_Location=u'E01032431', Vehicle_Type=9, Towing_and_Articulation=0, Vehicle_Manoeuvre=17, Vehicle_Location-Restricted_Lane=0, Junction_Location=0, Skidding_and_Overturning=0, Hit_Object_in_Carriageway=0, Vehicle_Leaving_Carriageway=0, Hit_Object_off_Carriageway=0, 1st_Point_of_Impact=1, Was_Vehicle_Left_Hand_Drive?=1, Journey_Purpose_of_Driver=15, Sex_of_Driver=1, Age_of_Driver=58, Age_Band_of_Driver=9, Engine_Capacity_(CC)=1799, Propulsion_Code=1, Age_of_Vehicle=7, Driver_IMD_Decile=1, Driver_Home_Area_Type=1, Casualty_Reference=1, Casualty_Class=1, Sex_of_Casualty=1, Age_of_Casualty=22, Age_Band_of_Casualty=5, Casualty_Severity=1, Pedestrian_Location=0, Pedestrian_Movement=0, Car_Passenger=0, Bus_or_Coach_Passenger=0, Pedestrian_Road_Maintenance_Worker=-1, Casualty_Type=4, Casualty_Home_Area_Type=1)]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "print(training_data.take(1))\n",
    "print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Accident_Index=u'200532D088205', Location_Easting_OSGR=498070, Location_Northing_OSGR=389760, Longitude=-0.526598, Latitude=53.395558, Police_Force=32, Accident_Severity=1, Number_of_Vehicles=2, Number_of_Casualties=9, Date=u'18/12/2005', Day_of_Week=1, Time=u'18:00', Local_Authority_(District)=356, Local_Authority_(Highway)=u'E10000019', 1st_Road_Class=3, 1st_Road_Number=631, Road_Type=6, Speed_limit=60, Junction_Detail=0, Junction_Control=-1, 2nd_Road_Class=-1, 2nd_Road_Number=0, Pedestrian_Crossing-Human_Control=0, Pedestrian_Crossing-Physical_Facilities=0, Light_Conditions=6, Weather_Conditions=1, Road_Surface_Conditions=1, Special_Conditions_at_Site=0, Carriageway_Hazards=0, Urban_or_Rural_Area=2, Did_Police_Officer_Attend_Scene_of_Accident=1, LSOA_of_Accident_Location=u'E01026411', Vehicle_Type=9, Towing_and_Articulation=0, Vehicle_Manoeuvre=16, Vehicle_Location-Restricted_Lane=0, Junction_Location=0, Skidding_and_Overturning=0, Hit_Object_in_Carriageway=0, Vehicle_Leaving_Carriageway=7, Hit_Object_off_Carriageway=0, 1st_Point_of_Impact=0, Was_Vehicle_Left_Hand_Drive?=1, Journey_Purpose_of_Driver=15, Sex_of_Driver=2, Age_of_Driver=57, Age_Band_of_Driver=9, Engine_Capacity_(CC)=1997, Propulsion_Code=2, Age_of_Vehicle=5, Driver_IMD_Decile=9, Driver_Home_Area_Type=1, Casualty_Reference=5, Casualty_Class=2, Sex_of_Casualty=1, Age_of_Casualty=22, Age_Band_of_Casualty=5, Casualty_Severity=3, Pedestrian_Location=0, Pedestrian_Movement=0, Car_Passenger=2, Bus_or_Coach_Passenger=0, Pedestrian_Road_Maintenance_Worker=-1, Casualty_Type=9, Casualty_Home_Area_Type=1)]\n"
     ]
    }
   ],
   "source": [
    "print(test_data.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #najprv si dropnem tie atributy ktore tam nechcem mat\n",
    "vzorka_cela = vzorka_cela.drop(\n",
    "\t\t\t\t\t \"Local_Authority_(Highway)\",\n",
    " \t\t\t\t\t \"LSOA_of_Accident_Location\",\n",
    "\t\t\t\t\t \"Time\",\n",
    "\t\t\t\t\t \"Date\",\n",
    "\t\t\t\t\t \"Accident_Index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Day_of_Week', 'Local_Authority_(District)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident', 'Vehicle_Type', 'Towing_and_Articulation', 'Vehicle_Manoeuvre', 'Vehicle_Location-Restricted_Lane', 'Junction_Location', 'Skidding_and_Overturning', 'Hit_Object_in_Carriageway', 'Vehicle_Leaving_Carriageway', 'Hit_Object_off_Carriageway', '1st_Point_of_Impact', 'Was_Vehicle_Left_Hand_Drive?', 'Journey_Purpose_of_Driver', 'Sex_of_Driver', 'Age_of_Driver', 'Age_Band_of_Driver', 'Engine_Capacity_(CC)', 'Propulsion_Code', 'Age_of_Vehicle', 'Driver_IMD_Decile', 'Driver_Home_Area_Type', 'Casualty_Reference', 'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', 'Casualty_Severity', 'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger', 'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type']\n"
     ]
    }
   ],
   "source": [
    "#do premennej names si ulozim mena atributov\n",
    "names = vzorka_cela.schema.names\n",
    "print(names)\n",
    "\n",
    "#print(names.count)\n",
    "#vytvorim si prazdny list correlations a do neho vo for cykle hodim korelacie kazdeho atributu s #atributom accident_severity, ale dajak to nefujguje zatial :D\n",
    "\n",
    "correlations = []\n",
    "for name in names:\n",
    "    correlations.extend([vzorka_cela.stat.corr('Accident_Severity',name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014786428056535588, -0.016216388447883277, 0.014881649693193248, -0.015886097377899137, -0.03761958333347941, 1.0, -0.06369110692696964, -0.12066636855785114, -0.0031986647821146815, -0.03773716887909407, 0.05359870288344656, 0.006687154872887726, -0.006109255069014051, -0.11069335025575405, 0.051871110687382184, 0.07414108021980696, 0.07442515761683713, 0.020083848899277185, 0.006897313588318904, 0.025252285899736885, -0.04905971443910559, -0.01099116368096592, -0.004272140184666045, 0.005466851521745526, -0.010512147510305134, -0.09896599141093332, 0.0565718636198449, -0.02126233930954931, -0.023882756534122994, -0.05971450692822329, -0.008074087360074823, 0.053147172978235346, -0.05192407596094767, -0.02176826954569437, -0.06959285988824376, -0.052997608668087025, 0.024864747604083595, 0.014137892928534054, -0.0034677890495713685, 0.037079759866389464, -0.031875756336561006, -0.031475758042892016, -0.04003647188244283, -0.012458325547380435, -0.006748442902452915, -0.0012217630387662988, -0.022888467298473648, -0.10583313411571098, -0.023487805802675704, 0.03299837863523214, -0.03493605456374568, -0.03103862302434451, 0.423926240717267, -0.015954331515356635, -0.018231709284581756, -0.017612278519246464, -0.00999043313345761, 0.011525705506401265, -0.018209370853050053, -0.0052982224372112975]\n"
     ]
    }
   ],
   "source": [
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|   Accident_Severity|                 1.0|\n",
      "|  Number_of_Vehicles|-0.06369110692696964|\n",
      "|Number_of_Casualties|-0.12066636855785114|\n",
      "|      1st_Road_Class| 0.05359870288344656|\n",
      "|         Speed_limit|-0.11069335025575405|\n",
      "|     Junction_Detail|0.051871110687382184|\n",
      "|    Junction_Control| 0.07414108021980696|\n",
      "|      2nd_Road_Class| 0.07442515761683713|\n",
      "| Urban_or_Rural_Area|-0.09896599141093332|\n",
      "|Did_Police_Office...|  0.0565718636198449|\n",
      "|   Vehicle_Manoeuvre|-0.05971450692822329|\n",
      "|   Junction_Location|0.053147172978235346|\n",
      "|Skidding_and_Over...|-0.05192407596094767|\n",
      "|Vehicle_Leaving_C...|-0.06959285988824376|\n",
      "|Hit_Object_off_Ca...|-0.05299760866808...|\n",
      "|  Casualty_Reference|-0.10583313411571098|\n",
      "|   Casualty_Severity|   0.423926240717267|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = vzorka_cela.schema.names\n",
    "t = zip(names,correlations)\n",
    "#print(t)\n",
    "#print(\"-------------------\")\n",
    "tt = spark.createDataFrame(t)\n",
    "#tt.show()\n",
    "\n",
    "tt.registerTempTable(\"TempTable\")\n",
    "atributy_table = sqlContext.sql('SELECT * FROM TempTable WHERE _2 > 0.05 OR _2 <-0.05')\n",
    "atributy = atributy_table.select(\"_2\")\n",
    "atributy_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214577"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vzorka_cela.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Accident_Severity=u'214577'),\n",
       " Row(summary=u'mean', Accident_Severity=u'1.9804126257707024'),\n",
       " Row(summary=u'stddev', Accident_Severity=u'0.13857777057006804'),\n",
       " Row(summary=u'min', Accident_Severity=u'1'),\n",
       " Row(summary=u'max', Accident_Severity=u'2')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accident_Severity = vzorka_cela.describe([\"Accident_Severity\"])\n",
    "Accident_Severity.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Number_of_Vehicles=u'214577'),\n",
       " Row(summary=u'mean', Number_of_Vehicles=u'2.363244895771681'),\n",
       " Row(summary=u'stddev', Number_of_Vehicles=u'2.5958718437844035'),\n",
       " Row(summary=u'min', Number_of_Vehicles=u'1'),\n",
       " Row(summary=u'max', Number_of_Vehicles=u'67')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Number_of_Vehicles = vzorka_cela.describe([\"Number_of_Vehicles\"])\n",
    "Number_of_Vehicles.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Number_of_Casualties=u'214577'),\n",
       " Row(summary=u'mean', Number_of_Casualties=u'2.1372374485615886'),\n",
       " Row(summary=u'stddev', Number_of_Casualties=u'3.446195182948457'),\n",
       " Row(summary=u'min', Number_of_Casualties=u'1'),\n",
       " Row(summary=u'max', Number_of_Casualties=u'93')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Number_of_Casualties = vzorka_cela.describe([\"Number_of_Casualties\"])\n",
    "Number_of_Casualties.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', 1st_Road_Class=u'214577'),\n",
       " Row(summary=u'mean', 1st_Road_Class=u'3.9091281917446885'),\n",
       " Row(summary=u'stddev', 1st_Road_Class=u'1.4513059991649084'),\n",
       " Row(summary=u'min', 1st_Road_Class=u'1'),\n",
       " Row(summary=u'max', 1st_Road_Class=u'6')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Road_Class = vzorka_cela.describe([\"1st_Road_Class\"])\n",
    "Road_Class.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Speed_limit=u'214577'),\n",
       " Row(summary=u'mean', Speed_limit=u'41.07348411059899'),\n",
       " Row(summary=u'stddev', Speed_limit=u'15.123157400089896'),\n",
       " Row(summary=u'min', Speed_limit=u'10'),\n",
       " Row(summary=u'max', Speed_limit=u'70')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Speed_limit = vzorka_cela.describe([\"Speed_limit\"])\n",
    "Speed_limit.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Junction_Detail=u'214577'),\n",
       " Row(summary=u'mean', Junction_Detail=u'2.376899667718348'),\n",
       " Row(summary=u'stddev', Junction_Detail=u'2.6154558806551593'),\n",
       " Row(summary=u'min', Junction_Detail=u'-1'),\n",
       " Row(summary=u'max', Junction_Detail=u'9')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Junction_Detail = vzorka_cela.describe([\"Junction_Detail\"])\n",
    "Junction_Detail.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Junction_Control=u'214577'),\n",
       " Row(summary=u'mean', Junction_Control=u'1.7916971530033508'),\n",
       " Row(summary=u'stddev', Junction_Control=u'2.302666453659238'),\n",
       " Row(summary=u'min', Junction_Control=u'-1'),\n",
       " Row(summary=u'max', Junction_Control=u'4')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Junction_Control = vzorka_cela.describe([\"Junction_Control\"])\n",
    "Junction_Control.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', 2nd_Road_Class=u'214577'),\n",
       " Row(summary=u'mean', 2nd_Road_Class=u'2.6265536380879593'),\n",
       " Row(summary=u'stddev', 2nd_Road_Class=u'3.211564003149304'),\n",
       " Row(summary=u'min', 2nd_Road_Class=u'-1'),\n",
       " Row(summary=u'max', 2nd_Road_Class=u'6')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd_Road_Class = vzorka_cela.describe([\"2nd_Road_Class\"])\n",
    "nd_Road_Class.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Urban_or_Rural_Area=u'214577'),\n",
       " Row(summary=u'mean', Urban_or_Rural_Area=u'1.4045261141688066'),\n",
       " Row(summary=u'stddev', Urban_or_Rural_Area=u'0.49095314546545527'),\n",
       " Row(summary=u'min', Urban_or_Rural_Area=u'1'),\n",
       " Row(summary=u'max', Urban_or_Rural_Area=u'3')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Urban_or_Rural_Area = vzorka_cela.describe([\"Urban_or_Rural_Area\"])\n",
    "Urban_or_Rural_Area.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Did_Police_Officer_Attend_Scene_of_Accident=u'214577'),\n",
       " Row(summary=u'mean', Did_Police_Officer_Attend_Scene_of_Accident=u'1.1567455971516052'),\n",
       " Row(summary=u'stddev', Did_Police_Officer_Attend_Scene_of_Accident=u'0.36963810518369733'),\n",
       " Row(summary=u'min', Did_Police_Officer_Attend_Scene_of_Accident=u'-1'),\n",
       " Row(summary=u'max', Did_Police_Officer_Attend_Scene_of_Accident=u'3')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Did_Police_Officer_Attend_Scene_of_Accident = vzorka_cela.describe([\"Did_Police_Officer_Attend_Scene_of_Accident\"])\n",
    "Did_Police_Officer_Attend_Scene_of_Accident.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Vehicle_Manoeuvre=u'214577'),\n",
       " Row(summary=u'mean', Vehicle_Manoeuvre=u'12.722766186497156'),\n",
       " Row(summary=u'stddev', Vehicle_Manoeuvre=u'6.176489972934277'),\n",
       " Row(summary=u'min', Vehicle_Manoeuvre=u'-1'),\n",
       " Row(summary=u'max', Vehicle_Manoeuvre=u'18')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vehicle_Manoeuvre = vzorka_cela.describe([\"Vehicle_Manoeuvre\"])\n",
    "Vehicle_Manoeuvre.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Junction_Location=u'214577'),\n",
       " Row(summary=u'mean', Junction_Location=u'2.4355219804545687'),\n",
       " Row(summary=u'stddev', Junction_Location=u'3.135052740159779'),\n",
       " Row(summary=u'min', Junction_Location=u'-1'),\n",
       " Row(summary=u'max', Junction_Location=u'8')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Junction_Location = vzorka_cela.describe([\"Junction_Location\"])\n",
    "Junction_Location.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Skidding_and_Overturning=u'214577'),\n",
       " Row(summary=u'mean', Skidding_and_Overturning=u'0.22709796483313682'),\n",
       " Row(summary=u'stddev', Skidding_and_Overturning=u'0.7270188381920063'),\n",
       " Row(summary=u'min', Skidding_and_Overturning=u'-1'),\n",
       " Row(summary=u'max', Skidding_and_Overturning=u'5')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Skidding_and_Overturning = vzorka_cela.describe([\"Skidding_and_Overturning\"])\n",
    "Skidding_and_Overturning.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Vehicle_Leaving_Carriageway=u'214577'),\n",
       " Row(summary=u'mean', Vehicle_Leaving_Carriageway=u'0.40655801879977815'),\n",
       " Row(summary=u'stddev', Vehicle_Leaving_Carriageway=u'1.4483770501859567'),\n",
       " Row(summary=u'min', Vehicle_Leaving_Carriageway=u'-1'),\n",
       " Row(summary=u'max', Vehicle_Leaving_Carriageway=u'8')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vehicle_Leaving_Carriageway = vzorka_cela.describe([\"Vehicle_Leaving_Carriageway\"])\n",
    "Vehicle_Leaving_Carriageway.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Hit_Object_off_Carriageway=u'214577'),\n",
       " Row(summary=u'mean', Hit_Object_off_Carriageway=u'0.604925038564245'),\n",
       " Row(summary=u'stddev', Hit_Object_off_Carriageway=u'2.1597066902557605'),\n",
       " Row(summary=u'min', Hit_Object_off_Carriageway=u'-1'),\n",
       " Row(summary=u'max', Hit_Object_off_Carriageway=u'11')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hit_Object_off_Carriageway = vzorka_cela.describe([\"Hit_Object_off_Carriageway\"])\n",
    "Hit_Object_off_Carriageway.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Casualty_Reference=u'214577'),\n",
       " Row(summary=u'mean', Casualty_Reference=u'1.573039048919502'),\n",
       " Row(summary=u'stddev', Casualty_Reference=u'2.024951480543275'),\n",
       " Row(summary=u'min', Casualty_Reference=u'1'),\n",
       " Row(summary=u'max', Casualty_Reference=u'91')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Casualty_Reference = vzorka_cela.describe([\"Casualty_Reference\"])\n",
    "Casualty_Reference.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary=u'count', Casualty_Severity=u'214577'),\n",
       " Row(summary=u'mean', Casualty_Severity=u'2.8802760780512355'),\n",
       " Row(summary=u'stddev', Casualty_Severity=u'0.353317973182534'),\n",
       " Row(summary=u'min', Casualty_Severity=u'1'),\n",
       " Row(summary=u'max', Casualty_Severity=u'3')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Casualty_Severity = vzorka_cela.describe([\"Casualty_Severity\"])\n",
    "Casualty_Severity.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Junction_Detail', 'Junction_Control', 'Did_Police_Officer_Attend_Scene_of_Accident', 'Junction_Location', 'Skidding_and_Overturning', 'Hit_Object_off_Carriageway']\n"
     ]
    }
   ],
   "source": [
    "cast_vzorky= vzorka_cela.drop(\"Location_Easting_OSGR\",\n",
    "\t\t\t\t\t \"Location_Northing_OSGR\",\n",
    "\t\t\t\t\t\"Longitude\",\n",
    "\t\t\t\t\t \"Latitude\",\n",
    "\t\t\t\t\t \"Local_Authority_(District)\",\n",
    "\t\t\t\t\t \"1st_Road_Class\",\n",
    "\t\t\t\t\t \"1st_Road_Number\",\n",
    "\t\t\t\t\t \"Road_Type\",\n",
    "\t\t\t\t\t \"Speed_limit\",\n",
    "\t\t\t\t\t \"2nd_Road_Class\",\n",
    "\t\t\t\t\t \"2nd_Road_Number\",\n",
    "\t\t\t\t\t\"Pedestrian_Crossing-Human_Control\",\n",
    "\t\t\t\t\t\"Pedestrian_Crossing-Physical_Facilities\",\n",
    "\t\t\t\t\t \"Special_Conditions_at_Site\",\n",
    "\t\t\t\t\t \"Carriageway_Hazards\",\n",
    "\t\t\t\t\t \"Urban_or_Rural_Area\",\n",
    "\t\t\t\t\t \"Vehicle_Reference\",\n",
    "\t\t\t\t\t \"Towing_and_Articulation\",\n",
    "\t\t\t\t\t \"Vehicle_Manoeuvre\",\n",
    "\t\t\t\t\t\"Vehicle_Location-Restricted_Lane\",\n",
    "\t\t\t\t\t \"Vehicle_Leaving_Carriageway\",\n",
    "\t\t\t\t\t \"Was_Vehicle_Left_Hand_Drive?\",\n",
    "\t\t\t\t\t \"Journey_Purpose_of_Driver\",\n",
    "\t\t\t\t\t \"Propulsion_Code\",\n",
    "\t\t\t\t\t \"Driver_IMD_Decile\",\n",
    "\t\t\t\t\t \"Driver_Home_Area_Type\",\n",
    "\t\t\t\t\t\"Vehicle_Reference\",\n",
    "\t\t\t\t\t \"Casualty_Reference\",\n",
    "\t\t\t\t\t \"Police_Force\",\n",
    "                     \"Day_of_Week\",\n",
    "                     \"Light_Conditions\",\n",
    "                     \"Weather_Conditions\",\n",
    "                     \"Road_Surface_Conditions\",\n",
    "                     \"Vehicle_Type\",\n",
    "                     \"Hit_Object_in_Carriageway\",\n",
    "                      \"1st_Point_of_Impact\",\n",
    "                     \"Sex_of_Driver\",\n",
    "                     \"Age_of_Driver\",\n",
    "                    \"Age_Band_of_Driver\",\n",
    "                    \"Engine_Capacity_(CC)\",\n",
    "                    \"Age_of_Vehicle\",\n",
    "                    \"Casualty_Class\",\n",
    "                    \"Sex_of_Casualty\",\n",
    "                    \"Age_of_Casualty\",\n",
    "                    \"Age_Band_of_Casualty\",\n",
    "                    \"Casualty_Severity\",\n",
    "                    \"Pedestrian_Location\",\n",
    "                    \"Pedestrian_Movement\",\n",
    "                    \"Car_Passenger\",\n",
    "                    \"Bus_or_Coach_Passenger\",\n",
    "                    \"Pedestrian_Road_Maintenance_Worker\",\n",
    "                    \"Casualty_Type\",\n",
    "                    \"Casualty_Home_Area_Type\")\n",
    "names = cast_vzorky.schema.names\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = cast_vzorky.randomSplit([0.6, 0.4], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128497"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelovanie\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_data = VectorAssembler(inputCols=[\"Number_of_Vehicles\", \"Number_of_Casualties\", \"Junction_Detail\", \"Junction_Control\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Junction_Location\",\"Skidding_and_Overturning\",\"Hit_Object_off_Carriageway\"],\n",
    "        outputCol=\"features\").transform(cast_vzorky)\n",
    "training_data, test_data = vector_data.randomSplit([0.7, 0.3], seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214577"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error: 0.0194\n"
     ]
    }
   ],
   "source": [
    "tree_classifier = DecisionTreeClassifier(featuresCol=\"features\",labelCol=\"Accident_Severity\",impurity=\"entropy\",maxDepth=10, maxBins=100) \n",
    " \n",
    "tree_model = tree_classifier.fit(training_data)\n",
    " \n",
    "predictions = tree_model.transform(test_data)\n",
    "\n",
    "#print(tree_model.toDebugString)\n",
    "test_error = predictions.filter(predictions[\"prediction\"] != predictions[\"Accident_Severity\"]).count() / float(test_data.count())\n",
    "print \"Testing error: {0:.4f}\".format(test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Junction_Detail', 'Junction_Control', 'Did_Police_Officer_Attend_Scene_of_Accident', 'Junction_Location', 'Skidding_and_Overturning', 'Hit_Object_off_Carriageway', 'features', 'rawPrediction', 'probability', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "print(predictions.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.05      0.09      1276\n",
      "           2       0.98      1.00      0.99     63261\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     64537\n",
      "   macro avg       0.80      0.53      0.54     64537\n",
      "weighted avg       0.97      0.98      0.97     64537\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   65,  1211],\n",
       "       [   39, 63222]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = predictions.select(['Accident_Severity']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score: ', 0.9806312657855184)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The ROC score is (@numTrees=200): ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = predictions.select(['probability', 'Accident_Severity'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is not binary and pos_label is not specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0518572f8132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 618\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\metrics\\ranking.pyc\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m              \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m              np.array_equal(classes, [1]))):\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data is not binary and pos_label is not specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data is not binary and pos_label is not specified"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128970.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       1.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "|       2.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4ad087bf8b795e1cda3c) of depth 10 with 767 nodes\n",
      "  If (feature 1 <= 3.0)\n",
      "   If (feature 4 <= 1.0)\n",
      "    If (feature 3 <= 0.0)\n",
      "     If (feature 1 <= 1.0)\n",
      "      If (feature 0 <= 1.0)\n",
      "       If (feature 6 <= 1.0)\n",
      "        If (feature 7 <= 3.0)\n",
      "         If (feature 5 <= -1.0)\n",
      "          If (feature 7 <= 0.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 0.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 5 > -1.0)\n",
      "          If (feature 7 <= 2.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 2.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 7 > 3.0)\n",
      "         If (feature 7 <= 4.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           If (feature 5 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > -1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 7 > 4.0)\n",
      "          If (feature 7 <= 5.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 7 > 5.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "       Else (feature 6 > 1.0)\n",
      "        If (feature 7 <= 0.0)\n",
      "         If (feature 6 <= 2.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 2 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 6 > 2.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 7 > 0.0)\n",
      "         If (feature 7 <= 1.0)\n",
      "          If (feature 6 <= 2.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 2.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 7 > 1.0)\n",
      "          If (feature 7 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 7 > 2.0)\n",
      "           If (feature 7 <= 4.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 4.0)\n",
      "            Predict: 2.0\n",
      "      Else (feature 0 > 1.0)\n",
      "       If (feature 0 <= 2.0)\n",
      "        If (feature 6 <= 0.0)\n",
      "         If (feature 7 <= 0.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 5 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           If (feature 5 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > -1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 7 > 0.0)\n",
      "          If (feature 7 <= 4.0)\n",
      "           If (feature 7 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 4.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 6 > 0.0)\n",
      "         If (feature 2 <= 0.0)\n",
      "          If (feature 7 <= 10.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 10.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 2 > 0.0)\n",
      "          If (feature 5 <= 1.0)\n",
      "           If (feature 5 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 0.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 5 > 1.0)\n",
      "           Predict: 2.0\n",
      "       Else (feature 0 > 2.0)\n",
      "        If (feature 0 <= 4.0)\n",
      "         If (feature 6 <= 0.0)\n",
      "          If (feature 7 <= 9.0)\n",
      "           If (feature 7 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 9.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 6 > 0.0)\n",
      "          If (feature 7 <= 0.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 0.0)\n",
      "           If (feature 7 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 9.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 0 > 4.0)\n",
      "         If (feature 7 <= 0.0)\n",
      "          If (feature 0 <= 6.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 6.0)\n",
      "           If (feature 0 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 9.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 7 > 0.0)\n",
      "          Predict: 2.0\n",
      "     Else (feature 1 > 1.0)\n",
      "      If (feature 7 <= 0.0)\n",
      "       If (feature 6 <= 2.0)\n",
      "        If (feature 1 <= 2.0)\n",
      "         If (feature 0 <= 5.0)\n",
      "          If (feature 6 <= 0.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 0.0)\n",
      "           If (feature 5 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 1.0)\n",
      "            Predict: 1.0\n",
      "         Else (feature 0 > 5.0)\n",
      "          If (feature 0 <= 8.0)\n",
      "           If (feature 0 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 8.0)\n",
      "           Predict: 2.0\n",
      "        Else (feature 1 > 2.0)\n",
      "         If (feature 0 <= 7.0)\n",
      "          If (feature 5 <= -1.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 5 > -1.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 0 > 7.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 0 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 9.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           If (feature 0 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 9.0)\n",
      "            Predict: 1.0\n",
      "       Else (feature 6 > 2.0)\n",
      "        If (feature 0 <= 1.0)\n",
      "         If (feature 1 <= 2.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 1 > 2.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           Predict: 2.0\n",
      "        Else (feature 0 > 1.0)\n",
      "         If (feature 1 <= 2.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 1 > 2.0)\n",
      "          If (feature 6 <= 3.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 3.0)\n",
      "           If (feature 5 <= -1.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 5 > -1.0)\n",
      "            Predict: 2.0\n",
      "      Else (feature 7 > 0.0)\n",
      "       If (feature 7 <= 4.0)\n",
      "        If (feature 1 <= 2.0)\n",
      "         If (feature 0 <= 6.0)\n",
      "          If (feature 0 <= 3.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 3.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 0 > 6.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           Predict: 1.0\n",
      "        Else (feature 1 > 2.0)\n",
      "         If (feature 5 <= -1.0)\n",
      "          Predict: 1.0\n",
      "         Else (feature 5 > -1.0)\n",
      "          If (feature 7 <= 3.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 3.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "       Else (feature 7 > 4.0)\n",
      "        If (feature 0 <= 1.0)\n",
      "         If (feature 7 <= 7.0)\n",
      "          If (feature 7 <= 6.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 6.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 7 > 7.0)\n",
      "          If (feature 5 <= -1.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 5 > -1.0)\n",
      "           If (feature 6 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 2.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 0 > 1.0)\n",
      "         If (feature 7 <= 7.0)\n",
      "          If (feature 6 <= 1.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 1.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 7 > 7.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           If (feature 7 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 9.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "    Else (feature 3 > 0.0)\n",
      "     If (feature 7 <= 0.0)\n",
      "      If (feature 2 <= 2.0)\n",
      "       If (feature 0 <= 1.0)\n",
      "        If (feature 5 <= 2.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 5 > 2.0)\n",
      "         If (feature 6 <= 1.0)\n",
      "          If (feature 2 <= 1.0)\n",
      "           If (feature 3 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 1.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 6 > 1.0)\n",
      "          Predict: 2.0\n",
      "       Else (feature 0 > 1.0)\n",
      "        If (feature 6 <= 1.0)\n",
      "         If (feature 5 <= 3.0)\n",
      "          If (feature 0 <= 4.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 4.0)\n",
      "           If (feature 2 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 5 > 3.0)\n",
      "          If (feature 1 <= 2.0)\n",
      "           If (feature 5 <= 4.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 4.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 1 > 2.0)\n",
      "           If (feature 5 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 7.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 6 > 1.0)\n",
      "         If (feature 1 <= 2.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 1 > 2.0)\n",
      "          If (feature 3 <= 2.0)\n",
      "           Predict: 1.0\n",
      "          Else (feature 3 > 2.0)\n",
      "           If (feature 5 <= 2.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 5 > 2.0)\n",
      "            Predict: 2.0\n",
      "      Else (feature 2 > 2.0)\n",
      "       If (feature 1 <= 1.0)\n",
      "        If (feature 0 <= 1.0)\n",
      "         If (feature 2 <= 6.0)\n",
      "          If (feature 5 <= 5.0)\n",
      "           If (feature 5 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 5 > 5.0)\n",
      "           If (feature 5 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 7.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 2 > 6.0)\n",
      "          If (feature 5 <= 5.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 5 > 5.0)\n",
      "           If (feature 2 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 7.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 0 > 1.0)\n",
      "         If (feature 3 <= 3.0)\n",
      "          If (feature 2 <= 3.0)\n",
      "           If (feature 5 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 3.0)\n",
      "           If (feature 2 <= 8.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 8.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 3 > 3.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           If (feature 2 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 6.0)\n",
      "            Predict: 2.0\n",
      "       Else (feature 1 > 1.0)\n",
      "        If (feature 6 <= 0.0)\n",
      "         If (feature 3 <= 2.0)\n",
      "          If (feature 0 <= 1.0)\n",
      "           If (feature 5 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 1.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 3 > 2.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           If (feature 5 <= 5.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 5.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 6 > 0.0)\n",
      "         If (feature 3 <= 1.0)\n",
      "          If (feature 1 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 1 > 2.0)\n",
      "           If (feature 0 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 1.0)\n",
      "            Predict: 1.0\n",
      "         Else (feature 3 > 1.0)\n",
      "          If (feature 6 <= 4.0)\n",
      "           If (feature 3 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 4.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 2.0\n",
      "     Else (feature 7 > 0.0)\n",
      "      If (feature 1 <= 2.0)\n",
      "       If (feature 2 <= 2.0)\n",
      "        If (feature 5 <= 7.0)\n",
      "         If (feature 5 <= 1.0)\n",
      "          If (feature 7 <= 9.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 7 > 9.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 5 > 1.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 5 > 7.0)\n",
      "         If (feature 6 <= 2.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 7 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 1.0\n",
      "         Else (feature 6 > 2.0)\n",
      "          If (feature 7 <= 1.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 1.0)\n",
      "           Predict: 2.0\n",
      "       Else (feature 2 > 2.0)\n",
      "        If (feature 0 <= 4.0)\n",
      "         If (feature 2 <= 5.0)\n",
      "          If (feature 6 <= 2.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 2.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 2 > 5.0)\n",
      "          If (feature 6 <= 0.0)\n",
      "           If (feature 5 <= 5.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 5.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 0.0)\n",
      "           If (feature 5 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 1.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 0 > 4.0)\n",
      "         If (feature 7 <= 4.0)\n",
      "          If (feature 5 <= 1.0)\n",
      "           If (feature 0 <= 5.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 0 > 5.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 5 > 1.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 7 > 4.0)\n",
      "          Predict: 2.0\n",
      "      Else (feature 1 > 2.0)\n",
      "       If (feature 5 <= 1.0)\n",
      "        If (feature 6 <= 1.0)\n",
      "         If (feature 2 <= 7.0)\n",
      "          If (feature 7 <= 3.0)\n",
      "           If (feature 2 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 3.0)\n",
      "           If (feature 0 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 2 > 7.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 6 > 1.0)\n",
      "         If (feature 0 <= 1.0)\n",
      "          If (feature 7 <= 4.0)\n",
      "           If (feature 2 <= 8.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 8.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 7 > 4.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 0 > 1.0)\n",
      "          If (feature 6 <= 3.0)\n",
      "           If (feature 7 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 6.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 6 > 3.0)\n",
      "           Predict: 2.0\n",
      "       Else (feature 5 > 1.0)\n",
      "        If (feature 2 <= 6.0)\n",
      "         If (feature 0 <= 3.0)\n",
      "          If (feature 6 <= 2.0)\n",
      "           If (feature 7 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 9.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 2.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 0 > 3.0)\n",
      "          If (feature 7 <= 1.0)\n",
      "           Predict: 1.0\n",
      "          Else (feature 7 > 1.0)\n",
      "           Predict: 2.0\n",
      "        Else (feature 2 > 6.0)\n",
      "         Predict: 2.0\n",
      "   Else (feature 4 > 1.0)\n",
      "    If (feature 7 <= 0.0)\n",
      "     If (feature 0 <= 1.0)\n",
      "      If (feature 3 <= -1.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 3 > -1.0)\n",
      "       If (feature 5 <= 2.0)\n",
      "        If (feature 2 <= 3.0)\n",
      "         If (feature 3 <= 3.0)\n",
      "          If (feature 2 <= 0.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 0.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 3 > 3.0)\n",
      "          If (feature 2 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 2 > 2.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 2 > 3.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 5 > 2.0)\n",
      "        Predict: 2.0\n",
      "     Else (feature 0 > 1.0)\n",
      "      If (feature 6 <= 2.0)\n",
      "       If (feature 0 <= 4.0)\n",
      "        If (feature 2 <= 3.0)\n",
      "         If (feature 0 <= 2.0)\n",
      "          If (feature 2 <= 2.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 2.0)\n",
      "           If (feature 1 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 0 > 2.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           Predict: 2.0\n",
      "        Else (feature 2 > 3.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 0 > 4.0)\n",
      "        If (feature 1 <= 1.0)\n",
      "         If (feature 3 <= -1.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 3 > -1.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 1 > 1.0)\n",
      "         Predict: 2.0\n",
      "      Else (feature 6 > 2.0)\n",
      "       If (feature 1 <= 1.0)\n",
      "        Predict: 2.0\n",
      "       Else (feature 1 > 1.0)\n",
      "        If (feature 2 <= 3.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 2 > 3.0)\n",
      "         If (feature 1 <= 2.0)\n",
      "          If (feature 5 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 5 > 2.0)\n",
      "           Predict: 1.0\n",
      "         Else (feature 1 > 2.0)\n",
      "          Predict: 2.0\n",
      "    Else (feature 7 > 0.0)\n",
      "     If (feature 0 <= 1.0)\n",
      "      Predict: 2.0\n",
      "     Else (feature 0 > 1.0)\n",
      "      If (feature 5 <= 1.0)\n",
      "       If (feature 7 <= 1.0)\n",
      "        If (feature 0 <= 2.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 0 > 2.0)\n",
      "         If (feature 2 <= 3.0)\n",
      "          Predict: 1.0\n",
      "         Else (feature 2 > 3.0)\n",
      "          Predict: 2.0\n",
      "       Else (feature 7 > 1.0)\n",
      "        If (feature 6 <= 0.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 6 > 0.0)\n",
      "         If (feature 7 <= 9.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 7 > 9.0)\n",
      "          If (feature 3 <= -1.0)\n",
      "           If (feature 1 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 2.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 3 > -1.0)\n",
      "           Predict: 2.0\n",
      "      Else (feature 5 > 1.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 1 > 3.0)\n",
      "   If (feature 1 <= 8.0)\n",
      "    If (feature 3 <= 0.0)\n",
      "     If (feature 7 <= 3.0)\n",
      "      If (feature 4 <= 1.0)\n",
      "       If (feature 1 <= 5.0)\n",
      "        If (feature 6 <= 0.0)\n",
      "         If (feature 0 <= 3.0)\n",
      "          If (feature 7 <= 1.0)\n",
      "           If (feature 5 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 1.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 0 > 3.0)\n",
      "          If (feature 1 <= 4.0)\n",
      "           If (feature 7 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 0.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 1 > 4.0)\n",
      "           If (feature 2 <= 8.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 8.0)\n",
      "            Predict: 1.0\n",
      "        Else (feature 6 > 0.0)\n",
      "         If (feature 2 <= 0.0)\n",
      "          If (feature 0 <= 1.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 1.0)\n",
      "           If (feature 6 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 2.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 2 > 0.0)\n",
      "          Predict: 1.0\n",
      "       Else (feature 1 > 5.0)\n",
      "        If (feature 0 <= 11.0)\n",
      "         If (feature 0 <= 9.0)\n",
      "          If (feature 0 <= 8.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 8.0)\n",
      "           If (feature 1 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 6.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 0 > 9.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 0 > 11.0)\n",
      "         If (feature 1 <= 6.0)\n",
      "          If (feature 0 <= 16.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 16.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 1 > 6.0)\n",
      "          Predict: 1.0\n",
      "      Else (feature 4 > 1.0)\n",
      "       If (feature 0 <= 4.0)\n",
      "        If (feature 3 <= -1.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 3 > -1.0)\n",
      "         If (feature 1 <= 4.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 1 > 4.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 1 <= 5.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 1 > 5.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           Predict: 2.0\n",
      "       Else (feature 0 > 4.0)\n",
      "        If (feature 0 <= 7.0)\n",
      "         If (feature 1 <= 4.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 1 > 4.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 0 > 7.0)\n",
      "         If (feature 1 <= 4.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 1 > 4.0)\n",
      "          Predict: 1.0\n",
      "     Else (feature 7 > 3.0)\n",
      "      If (feature 7 <= 4.0)\n",
      "       If (feature 0 <= 6.0)\n",
      "        If (feature 1 <= 7.0)\n",
      "         If (feature 1 <= 6.0)\n",
      "          If (feature 1 <= 4.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 1 > 4.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 1 > 6.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 0 <= 1.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 0 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           Predict: 1.0\n",
      "        Else (feature 1 > 7.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 0 > 6.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 7 > 4.0)\n",
      "       If (feature 1 <= 6.0)\n",
      "        If (feature 7 <= 9.0)\n",
      "         If (feature 6 <= 3.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 1 <= 4.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 4.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           If (feature 7 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 6.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 6 > 3.0)\n",
      "          If (feature 0 <= 1.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 0 > 1.0)\n",
      "           If (feature 7 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 7.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 7 > 9.0)\n",
      "         If (feature 1 <= 4.0)\n",
      "          If (feature 7 <= 10.0)\n",
      "           If (feature 0 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 10.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 1 > 4.0)\n",
      "          If (feature 6 <= 3.0)\n",
      "           If (feature 5 <= -1.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 5 > -1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 3.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 1.0\n",
      "       Else (feature 1 > 6.0)\n",
      "        If (feature 0 <= 9.0)\n",
      "         If (feature 0 <= 7.0)\n",
      "          If (feature 5 <= -1.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 5 > -1.0)\n",
      "           If (feature 3 <= -1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 3 > -1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 0 > 7.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 0 > 9.0)\n",
      "         Predict: 1.0\n",
      "    Else (feature 3 > 0.0)\n",
      "     If (feature 6 <= 0.0)\n",
      "      If (feature 2 <= 2.0)\n",
      "       If (feature 0 <= 3.0)\n",
      "        If (feature 5 <= 1.0)\n",
      "         If (feature 1 <= 4.0)\n",
      "          If (feature 0 <= 2.0)\n",
      "           If (feature 4 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 4 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 2.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 1 > 4.0)\n",
      "          Predict: 2.0\n",
      "        Else (feature 5 > 1.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 0 > 3.0)\n",
      "        If (feature 1 <= 4.0)\n",
      "         Predict: 2.0\n",
      "        Else (feature 1 > 4.0)\n",
      "         If (feature 3 <= 2.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 3 > 2.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           If (feature 5 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           Predict: 2.0\n",
      "      Else (feature 2 > 2.0)\n",
      "       If (feature 7 <= 0.0)\n",
      "        If (feature 4 <= 1.0)\n",
      "         If (feature 2 <= 8.0)\n",
      "          If (feature 0 <= 4.0)\n",
      "           If (feature 1 <= 5.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 5.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 4.0)\n",
      "           If (feature 5 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 1.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 2 > 8.0)\n",
      "          If (feature 3 <= 3.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 3 > 3.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 4 > 1.0)\n",
      "         If (feature 5 <= 5.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 5 > 5.0)\n",
      "          If (feature 2 <= 5.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 2 > 5.0)\n",
      "           If (feature 2 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 6.0)\n",
      "            Predict: 2.0\n",
      "       Else (feature 7 > 0.0)\n",
      "        If (feature 2 <= 7.0)\n",
      "         If (feature 7 <= 6.0)\n",
      "          If (feature 7 <= 2.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 2.0)\n",
      "           If (feature 1 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 6.0)\n",
      "            Predict: 1.0\n",
      "         Else (feature 7 > 6.0)\n",
      "          If (feature 0 <= 1.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 0 > 1.0)\n",
      "           If (feature 7 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 7.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 2 > 7.0)\n",
      "         If (feature 1 <= 5.0)\n",
      "          If (feature 7 <= 10.0)\n",
      "           If (feature 7 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 1.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 7 > 10.0)\n",
      "           If (feature 5 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 2.0)\n",
      "            Predict: 1.0\n",
      "         Else (feature 1 > 5.0)\n",
      "          Predict: 2.0\n",
      "     Else (feature 6 > 0.0)\n",
      "      If (feature 1 <= 5.0)\n",
      "       If (feature 0 <= 4.0)\n",
      "        If (feature 7 <= 1.0)\n",
      "         If (feature 5 <= 5.0)\n",
      "          If (feature 2 <= 3.0)\n",
      "           If (feature 5 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 5 > 2.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 3.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 5 > 5.0)\n",
      "          If (feature 2 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 2 > 2.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 7 > 1.0)\n",
      "         If (feature 5 <= 1.0)\n",
      "          If (feature 1 <= 4.0)\n",
      "           If (feature 0 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 3.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 1 > 4.0)\n",
      "           If (feature 2 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 3.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 5 > 1.0)\n",
      "          If (feature 2 <= 7.0)\n",
      "           If (feature 7 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 2 > 7.0)\n",
      "           If (feature 6 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 6 > 1.0)\n",
      "            Predict: 1.0\n",
      "       Else (feature 0 > 4.0)\n",
      "        If (feature 5 <= 2.0)\n",
      "         If (feature 2 <= 1.0)\n",
      "          Predict: 1.0\n",
      "         Else (feature 2 > 1.0)\n",
      "          If (feature 3 <= 2.0)\n",
      "           Predict: 1.0\n",
      "          Else (feature 3 > 2.0)\n",
      "           If (feature 0 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 6.0)\n",
      "            Predict: 1.0\n",
      "        Else (feature 5 > 2.0)\n",
      "         Predict: 2.0\n",
      "      Else (feature 1 > 5.0)\n",
      "       If (feature 6 <= 1.0)\n",
      "        If (feature 2 <= 5.0)\n",
      "         If (feature 5 <= 1.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           If (feature 2 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 3.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 5 > 1.0)\n",
      "          If (feature 5 <= 2.0)\n",
      "           If (feature 1 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 5 > 2.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 2 > 5.0)\n",
      "         If (feature 2 <= 6.0)\n",
      "          If (feature 0 <= 3.0)\n",
      "           If (feature 7 <= 9.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 7 > 9.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 0 > 3.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 2 > 6.0)\n",
      "          If (feature 2 <= 8.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 2 > 8.0)\n",
      "           If (feature 0 <= 2.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 2.0)\n",
      "            Predict: 2.0\n",
      "       Else (feature 6 > 1.0)\n",
      "        If (feature 2 <= 5.0)\n",
      "         If (feature 6 <= 3.0)\n",
      "          If (feature 0 <= 3.0)\n",
      "           If (feature 4 <= 1.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 4 > 1.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 0 > 3.0)\n",
      "           Predict: 1.0\n",
      "         Else (feature 6 > 3.0)\n",
      "          If (feature 2 <= 2.0)\n",
      "           Predict: 2.0\n",
      "          Else (feature 2 > 2.0)\n",
      "           If (feature 1 <= 6.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 6.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 2 > 5.0)\n",
      "         Predict: 2.0\n",
      "   Else (feature 1 > 8.0)\n",
      "    If (feature 0 <= 34.0)\n",
      "     If (feature 0 <= 22.0)\n",
      "      If (feature 1 <= 21.0)\n",
      "       If (feature 2 <= 3.0)\n",
      "        If (feature 0 <= 14.0)\n",
      "         If (feature 0 <= 8.0)\n",
      "          If (feature 6 <= 3.0)\n",
      "           If (feature 0 <= 7.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 0 > 7.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 6 > 3.0)\n",
      "           If (feature 1 <= 13.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 1 > 13.0)\n",
      "            Predict: 2.0\n",
      "         Else (feature 0 > 8.0)\n",
      "          If (feature 0 <= 9.0)\n",
      "           If (feature 6 <= 0.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 6 > 0.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 9.0)\n",
      "           If (feature 2 <= 0.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 0.0)\n",
      "            Predict: 1.0\n",
      "        Else (feature 0 > 14.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 2 > 3.0)\n",
      "        If (feature 0 <= 7.0)\n",
      "         If (feature 1 <= 9.0)\n",
      "          Predict: 2.0\n",
      "         Else (feature 1 > 9.0)\n",
      "          If (feature 0 <= 5.0)\n",
      "           If (feature 1 <= 13.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 1 > 13.0)\n",
      "            Predict: 2.0\n",
      "          Else (feature 0 > 5.0)\n",
      "           Predict: 1.0\n",
      "        Else (feature 0 > 7.0)\n",
      "         Predict: 1.0\n",
      "      Else (feature 1 > 21.0)\n",
      "       If (feature 0 <= 5.0)\n",
      "        If (feature 1 <= 70.0)\n",
      "         If (feature 1 <= 47.0)\n",
      "          If (feature 2 <= 5.0)\n",
      "           If (feature 2 <= 3.0)\n",
      "            Predict: 2.0\n",
      "           Else (feature 2 > 3.0)\n",
      "            Predict: 1.0\n",
      "          Else (feature 2 > 5.0)\n",
      "           Predict: 2.0\n",
      "         Else (feature 1 > 47.0)\n",
      "          If (feature 2 <= 0.0)\n",
      "           Predict: 1.0\n",
      "          Else (feature 2 > 0.0)\n",
      "           If (feature 0 <= 1.0)\n",
      "            Predict: 1.0\n",
      "           Else (feature 0 > 1.0)\n",
      "            Predict: 2.0\n",
      "        Else (feature 1 > 70.0)\n",
      "         Predict: 2.0\n",
      "       Else (feature 0 > 5.0)\n",
      "        Predict: 1.0\n",
      "     Else (feature 0 > 22.0)\n",
      "      If (feature 1 <= 38.0)\n",
      "       If (feature 1 <= 17.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 1 > 17.0)\n",
      "        Predict: 2.0\n",
      "      Else (feature 1 > 38.0)\n",
      "       Predict: 1.0\n",
      "    Else (feature 0 > 34.0)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree_model.toDebugString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o184.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 153.0 failed 1 times, most recent failure: Lost task 3.0 in stage 153.0 (TID 13043, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\3b\\temp_shuffle_be5a30cc-238c-4c19-b8a1-93fb42782f3c (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:191)\r\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:75)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\3b\\temp_shuffle_be5a30cc-238c-4c19-b8a1-93fb42782f3c (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e13f055961a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         labelCol=\"Accident_Severity\")                  \n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msvm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o184.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 153.0 failed 1 times, most recent failure: Lost task 3.0 in stage 153.0 (TID 13043, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\3b\\temp_shuffle_be5a30cc-238c-4c19-b8a1-93fb42782f3c (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\r\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:191)\r\n\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:75)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\3b\\temp_shuffle_be5a30cc-238c-4c19-b8a1-93fb42782f3c (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#SVM - nechapem prečo hadže chybu že SVM robi iba binarnu klasifikaciu a naš cieľovy #atribut ma 3 triedy - nema lebo som to upravil a ma len 2 takže nechapem fakt ..\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "vzorka_cela.withColumn(\"Accident_Severity\", when(vzorka_cela[\"Accident_Severity\"] == -1, 2).otherwise(vzorka_cela[\"Accident_Severity\"]))\n",
    "vector_data = VectorAssembler(inputCols=[\"Number_of_Vehicles\", \"Number_of_Casualties\", \"Junction_Detail\", \"Junction_Control\", \"Did_Police_Officer_Attend_Scene_of_Accident\", \"Junction_Location\",\"Skidding_and_Overturning\",\"Hit_Object_off_Carriageway\"],\n",
    "        outputCol=\"features\").transform(vzorka_cela)\n",
    "svm_classifier = LinearSVC(\n",
    "        featuresCol=\"features\",             \n",
    "        labelCol=\"Accident_Severity\")                  \n",
    "\n",
    "svm_model = svm_classifier.fit(training_data)\n",
    "\n",
    "predictions = svm_model.transform(test_data)\n",
    "\n",
    "test_error = predictions.filter(predictions[\"prediction\"] != predictions[\"Accident_Severity\"]).count() / float(test_data.count())\n",
    "print \"Testing error: {0:.4f}\".format(test_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o558.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 179.0 failed 1 times, most recent failure: Lost task 1.0 in stage 179.0 (TID 13086, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\05\\temp_shuffle_462463b0-2791-422e-9a38-fdd24acbcadb (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2160)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:154)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\05\\temp_shuffle_462463b0-2791-422e-9a38-fdd24acbcadb (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-5915ac9c6402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o558.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 179.0 failed 1 times, most recent failure: Lost task 1.0 in stage 179.0 (TID 13086, localhost, executor driver): java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\05\\temp_shuffle_462463b0-2791-422e-9a38-fdd24acbcadb (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2160)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:154)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.base/java.lang.Thread.run(Thread.java:844)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\FIlip\\AppData\\Local\\Temp\\blockmgr-4b3fb485-f64f-4b1d-94ab-bc56b0b66c9b\\05\\temp_shuffle_462463b0-2791-422e-9a38-fdd24acbcadb (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:299)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:238)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# bayes- haže chybu že nevie pracovať s negativnymi hodnotami - stale to for nemame ak #tak to možem spraviť aj ručne na tych par atributov čo nam ostalo to nebude problem... \n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    " \n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",featuresCol=\"features\", labelCol=\"Accident_Severity\")\n",
    " \n",
    "# train the model\n",
    "model = nb.fit(training_data)\n",
    " \n",
    "predictions = model.transform(test_data)\n",
    "#predictions.show()\n",
    " \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Accident_Severity\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error: 0.0196\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Accident_Severity\", featuresCol=\"features\",impurity=\"entropy\", numTrees=10, maxBins=100)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "#print(model.toDebugString)`\n",
    "\n",
    "test_error = predictions.filter(predictions[\"prediction\"] != predictions[\"Accident_Severity\"]).count() / float(test_data.count())\n",
    "print \"Testing error: {0:.4f}\".format(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.01      0.01      1276\n",
      "           2       0.98      1.00      0.99     63261\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     64537\n",
      "   macro avg       0.99      0.50      0.50     64537\n",
      "weighted avg       0.98      0.98      0.97     64537\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    9,  1267],\n",
       "       [    0, 63261]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vyhodnotenie - RandomForest \n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "y_true = predictions.select(['Accident_Severity']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print classification_report(y_true, y_pred)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score: ', 0.9803678510002014)\n"
     ]
    }
   ],
   "source": [
    "#presnost RandomForest \n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'Field \"features\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-23400670f440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Train model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbt_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Make predictions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\ml\\wrapper.pyc\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \"\"\"\n\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: u'Field \"features\" does not exist.'"
     ]
    }
   ],
   "source": [
    "# Gradient boosted trees -vychadza Testing error: 1.0000\n",
    "#Root Mean Squared Error (RMSE) on test data = 0.134178\n",
    "#netušim či to je dobre … testing error 1 znamena že 100% chyba  ? :D \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "gbt_reg = GBTRegressor(featuresCol=\"features\", labelCol=\"Accident_Severity\", maxIter=10, maxBins=100)\n",
    " \n",
    "# Train model. \n",
    "model = gbt_reg.fit(training_data)\n",
    " \n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    " \n",
    "#test_error = predictions.filter(predictions[\"prediction\"] != predictions[\"Accident_Severity\"]).count() / float(test_data.count())\n",
    "#print \"Testing error: {0:.4f}\".format(test_error)\n",
    " \n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Accident_Severity\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score: ', 0.9803678510002014)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-cac6caf49743>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     \"\"\"\n\u001b[0;32m   1523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "#vyhodnotenie - Gradient boosted trees\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "y_true = predictions.select(['Accident_Severity']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print classification_report(y_true, y_pred)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 3067387.30004\n",
      "------------------------------------------------\n",
      "Cluster Centers: \n",
      "[2.16279465 1.86458322 4.02222398 3.63316833 1.17691968 4.13475981\n",
      " 0.14607569 0.35833626]\n",
      "[ 2.43323572  2.22355621  0.01776476 -0.83660322  1.1295754   0.00875224\n",
      "  0.34268749  0.96650399]\n",
      "[50.06968641 67.59233449  0.11498258 -0.84320557  1.          0.15679443\n",
      "  0.22648084  0.42160279]\n",
      "------------------------------------------------\n",
      "anomalia: 50.069686411149824\n",
      "anomalia: 67.592334494773525\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "#Trains a k-means model.\n",
    "kmeans = KMeans().setK(3).setSeed(1234)\n",
    "model = kmeans.fit(training_data)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(training_data)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "print(\"------------------------------------------------\")\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)    \n",
    "    \n",
    "print(\"------------------------------------------------\")\n",
    "#detekovanie anomalii\n",
    "for center in centers:\n",
    "    for point in center:\n",
    "        if (point > 5 or -5 > point):\n",
    "               print \"anomalia: {0:.15f}\".format(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.03      0.67      0.06      1276\n",
      "           2       0.72      0.00      0.00     63261\n",
      "\n",
      "   micro avg       0.01      0.01      0.01     64537\n",
      "   macro avg       0.25      0.22      0.02     64537\n",
      "weighted avg       0.70      0.01      0.00     64537\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0],\n",
       "       [  389,   851,    36],\n",
       "       [37516, 25654,    91]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vyhodnotenie-K-means\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "y_true = predictions.select(['Accident_Severity']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy score: ', 0.014596278104033345)\n"
     ]
    }
   ],
   "source": [
    "#presnost K-means\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
